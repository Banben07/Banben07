{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECG",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12Jy5UdJ94gQxqk5I1iNq9vzCpBmMqGc_",
      "authorship_tag": "ABX9TyPCEYdtDB07nJNR0xM+cOCf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Banben07/Banben07/blob/main/ECG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyts\n",
        "!pip install wfdb"
      ],
      "metadata": {
        "id": "aSr-w-mIMbGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhdwLqw4E3Fz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "import wfdb\n",
        "import pywt\n",
        "import seaborn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# 项目目录\n",
        "project_path = '/content/drive/MyDrive/ECG_Classification'\n",
        "# 定义日志目录,必须是启动web应用时指定目录的子目录,建议使用日期时间作为子目录名\n",
        "log_dir = project_path + \"logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "model_path = project_path + \"ecg_model.h5\"\n",
        "\n",
        "# 测试集在数据集中所占的比例\n",
        "RATIO = 0.3\n",
        "\n",
        "\n",
        "# 小波去噪预处理\n",
        "def denoise(data):\n",
        "    # 小波变换\n",
        "    coeffs = pywt.wavedec(data=data, wavelet='db5', level=9)\n",
        "    cA9, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs\n",
        "\n",
        "    # 阈值去噪\n",
        "    threshold = (np.median(np.abs(cD1)) / 0.6745) * (np.sqrt(2 * np.log(len(cD1))))\n",
        "    cD1.fill(0)\n",
        "    cD2.fill(0)\n",
        "    for i in range(1, len(coeffs) - 2):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], threshold)\n",
        "\n",
        "    # 小波反变换,获取去噪后的信号\n",
        "    rdata = pywt.waverec(coeffs=coeffs, wavelet='db5')\n",
        "\n",
        "    return rdata\n",
        "\n",
        "\n",
        "# 读取心电数据和对应标签,并对数据进行小波去噪\n",
        "def getDataSet(number, X_data, Y_data):\n",
        "    ecgClassSet = ['N', 'A', 'V', 'L', 'R']\n",
        "\n",
        "    # 读取心电数据记录\n",
        "    print(\"正在读取\" + number + \" 号心电数据...\")\n",
        "    record = wfdb.rdrecord('/content/drive/MyDrive/ECG_Classification/ecg_data/' + number, channel_names=['MLII'])\n",
        "    data = record.p_signal.flatten()\n",
        "    rdata = denoise(data=data)\n",
        "\n",
        "    # 获取心电数据记录中R波的位置和对应的标签\n",
        "    annotation = wfdb.rdann('/content/drive/MyDrive/ECG_Classification/ecg_data/' + number, 'atr')\n",
        "    Rlocation = annotation.sample\n",
        "    Rclass = annotation.symbol\n",
        "\n",
        "    # 去掉前后的不稳定数据\n",
        "    start = 10\n",
        "    end = 5\n",
        "    i = start\n",
        "    j = len(annotation.symbol) - end\n",
        "\n",
        "    # 因为只选择NAVLR五种心电类型,所以要选出该条记录中所需要的那些带有特定标签的数据,舍弃其余标签的点\n",
        "    # X_data在R波前后截取长度为300的数据点\n",
        "    # Y_data将NAVLR按顺序转换为01234\n",
        "    while i < j:\n",
        "        try:\n",
        "            lable = ecgClassSet.index(Rclass[i])\n",
        "            x_train = rdata[Rlocation[i] - 99:Rlocation[i] + 201]\n",
        "            X_data.append(x_train)\n",
        "            Y_data.append(lable)\n",
        "            i += 1\n",
        "        except ValueError:\n",
        "            i += 1\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# 加载数据集并进行预处理\n",
        "def loadData():\n",
        "    numberSet = ['100', '101', '103', '105', '106', '107', '108', '109', '111', '112', '113', '114', '115',\n",
        "                 '116', '117', '119', '121', '122', '123', '124', '200', '201', '202', '203', '205', '208',\n",
        "                 '210', '212', '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230',\n",
        "                 '231', '232', '233', '234']\n",
        "    dataSet = []\n",
        "    lableSet = []\n",
        "    for n in numberSet:\n",
        "        getDataSet(n, dataSet, lableSet)\n",
        "\n",
        "    # 转numpy数组,打乱顺序\n",
        "    dataSet = np.array(dataSet).reshape(-1, 300)\n",
        "    lableSet = np.array(lableSet).reshape(-1, 1)\n",
        "    train_ds = np.hstack((dataSet, lableSet))\n",
        "    np.random.shuffle(train_ds)\n",
        "\n",
        "    # 数据集及其标签集\n",
        "    X = train_ds[:, :300].reshape(-1, 300, 1)\n",
        "    Y = train_ds[:, 300]\n",
        "\n",
        "    # 测试集及其标签集\n",
        "    shuffle_index = np.random.permutation(len(X))\n",
        "    test_length = int(RATIO * len(shuffle_index))\n",
        "    test_index = shuffle_index[:test_length]\n",
        "    train_index = shuffle_index[test_length:]\n",
        "    X_test, Y_test = X[test_index], Y[test_index]\n",
        "    X_train, Y_train = X[train_index], Y[train_index]\n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "\n",
        "# 构建CNN模型\n",
        "def buildModel():\n",
        "    newModel = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(300, 1)),\n",
        "        # 第一个卷积层, 4 个 21x1 卷积核\n",
        "        tf.keras.layers.Conv1D(filters=4, kernel_size=21, strides=1, padding='SAME', activation='relu'),\n",
        "        # 第一个池化层, 最大池化,4 个 3x1 卷积核, 步长为 2\n",
        "        tf.keras.layers.MaxPool1D(pool_size=3, strides=2, padding='SAME'),\n",
        "        # 第二个卷积层, 16 个 23x1 卷积核\n",
        "        tf.keras.layers.Conv1D(filters=16, kernel_size=23, strides=1, padding='SAME', activation='relu'),\n",
        "        # 第二个池化层, 最大池化,4 个 3x1 卷积核, 步长为 2\n",
        "        tf.keras.layers.MaxPool1D(pool_size=3, strides=2, padding='SAME'),\n",
        "        # 第三个卷积层, 32 个 25x1 卷积核\n",
        "        tf.keras.layers.Conv1D(filters=32, kernel_size=25, strides=1, padding='SAME', activation='relu'),\n",
        "        # 第三个池化层, 平均池化,4 个 3x1 卷积核, 步长为 2\n",
        "        tf.keras.layers.AvgPool1D(pool_size=3, strides=2, padding='SAME'),\n",
        "        # 第四个卷积层, 64 个 27x1 卷积核\n",
        "        tf.keras.layers.Conv1D(filters=64, kernel_size=27, strides=1, padding='SAME', activation='relu'),\n",
        "        # 打平层,方便全连接层处理\n",
        "        tf.keras.layers.Flatten(),\n",
        "        # 全连接层,128 个节点\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        # Dropout层,dropout = 0.2\n",
        "        tf.keras.layers.Dropout(rate=0.2),\n",
        "        # 全连接层,5 个节点\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "    return newModel\n",
        "\n",
        "\n",
        "# 混淆矩阵\n",
        "def plotHeatMap(Y_test, Y_pred):\n",
        "    con_mat = confusion_matrix(Y_test, Y_pred)\n",
        "    # 归一化\n",
        "    # con_mat_norm = con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis]\n",
        "    # con_mat_norm = np.around(con_mat_norm, decimals=2)\n",
        "\n",
        "    # 绘图\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    seaborn.heatmap(con_mat, annot=True, fmt='.20g', cmap='Blues')\n",
        "    plt.ylim(0, 5)\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # X_train,Y_train为所有的数据集和标签集\n",
        "    # X_test,Y_test为拆分的测试集和标签集\n",
        "    X_train, Y_train, X_test, Y_test = loadData()\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        # 导入训练好的模型\n",
        "        model = tf.keras.models.load_model(filepath=model_path)\n",
        "    else:\n",
        "        # 构建CNN模型\n",
        "        model = buildModel()\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        model.summary()\n",
        "        # 定义TensorBoard对象\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "        # 训练与验证\n",
        "        model.fit(X_train, Y_train, epochs=30,\n",
        "                  batch_size=128,\n",
        "                  validation_split=RATIO,\n",
        "                  callbacks=[tensorboard_callback])\n",
        "        model.save(filepath=model_path)\n",
        "\n",
        "    # 预测\n",
        "\n",
        "    predict_x = model.predict(X_test) \n",
        "    Y_pred = np.argmax(predict_x,axis=1)\n",
        "    # 绘制混淆矩阵\n",
        "    plotHeatMap(Y_test, Y_pred)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "做去噪效果好坏的评价"
      ],
      "metadata": {
        "id": "i5Vn3fAV2dR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取编号为data的一条心电数据\n",
        "import pywt\n",
        "import wfdb\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from pyts.image import GramianAngularField\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def OneDenoise(data):\n",
        "    # 一维小波变换\n",
        "    coeffs = pywt.wavedec(data=data, wavelet='db5', level=9)  # wavedec()函数完成1D多阶次离散小波分解,\n",
        "    # 返回系数数组list，[cAn, cDn, cDn-1, …, cD2, cD1]，n为分解阶次，cAn是逼近系数数组，后面的依次是细节系数数组。\n",
        "    cA9, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs\n",
        "\n",
        "    # 阈值去噪\n",
        "    threshold = (np.median(np.abs(cD1)) / 0.6745) * (np.sqrt(2 * np.log(len(cD1))))\n",
        "    cD1.fill(0)\n",
        "    cD2.fill(0)\n",
        "    for i in range(1, len(coeffs) - 2):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], threshold)\n",
        "\n",
        "    # 小波反变换,获取去噪后的信号\n",
        "    rdata = pywt.waverec(coeffs=coeffs, wavelet='db5')\n",
        "\n",
        "    return rdata\n",
        "\n",
        "\n",
        "def TwoDenoise(in_data):\n",
        "    w = 'sym4'  # 定义小波基的类型\n",
        "    l = 3  # 变换层次为3\n",
        "    coeffs = pywt.wavedec2(data=in_data, wavelet=w, level=l)  # 对图像进行小波分解\n",
        "    threshold = 0.04\n",
        "\n",
        "    list_coeffs = []\n",
        "    for i in range(1, len(coeffs)):\n",
        "        list_coeffs_ = list(coeffs[i])\n",
        "        list_coeffs.append(list_coeffs_)\n",
        "\n",
        "    for r1 in range(len(list_coeffs)):\n",
        "        for r2 in range(len(list_coeffs[r1])):\n",
        "            # 对噪声滤波(软阈值)\n",
        "            list_coeffs[r1][r2] = \\\n",
        "                pywt.threshold(list_coeffs[r1][r2], threshold * np.max(list_coeffs[r1][r2]))\n",
        "\n",
        "    rec_coeffs = [coeffs[0]]  # 重构系数\n",
        "\n",
        "    for j in range(len(list_coeffs)):\n",
        "        rec_coeffs_ = tuple(list_coeffs[j])\n",
        "        rec_coeffs.append(rec_coeffs_)\n",
        "\n",
        "    rdata2 = pywt.waverec2(rec_coeffs, 'sym4')\n",
        "\n",
        "    return rdata2\n",
        "\n",
        "def SNR_singlech(S, SN):\n",
        "    S = S-np.mean(S)# 消除直流分量\n",
        "    S = S/np.max(np.abs(S))#幅值归一化\n",
        "    mean_S = (np.sum(S))/(len(S))#纯信号的平均值\n",
        "    PS = np.sum((S-mean_S)*(S-mean_S))\n",
        "    PN = np.sum((S-SN)*(S-SN))\n",
        "    snr=10*math.log((PS/PN), 10)\n",
        "    return(snr)\n",
        "\n",
        "def GramTrans(in_data):\n",
        "    # 一维信号二维图像化\n",
        "    bi_data = in_data.reshape(1, -1)\n",
        "    image_size = 100\n",
        "\n",
        "    # 使用summation方法\n",
        "    gasf = GramianAngularField(image_size=image_size, method='summation')\n",
        "    out_gasf = gasf.fit_transform(bi_data)\n",
        "\n",
        "    return out_gasf\n",
        "\n",
        "\n",
        "def read_ecg_data(data):\n",
        "    \"\"\"\n",
        "    读取心电信号文件\n",
        "    sampfrom: 设置读取心电信号的起始位置，sampfrom=0表示从0开始读取，默认从0开始\n",
        "    sampto：设置读取心电信号的结束位置，sampto = 1500表示从1500出结束，默认读到文件末尾\n",
        "    channel_names：设置设置读取心电信号名字，必须是列表，channel_names=['MLII']表示读取MLII导联线\n",
        "    channels：设置读取第几个心电信号，必须是列表，channels=[0, 3]表示读取第0和第3个信号，注意信号数不确定\n",
        "    \"\"\"\n",
        "    data_path = \"/content/drive/MyDrive/ECG_Classification/ecg_data/\"\n",
        "    # 仅仅读取“MLII”导联的信号\n",
        "    print(\"正在读取 \" + data + \" 号心电数据...\")\n",
        "    record = wfdb.rdrecord(data_path + data, sampfrom=0, sampto=1500, channel_names=['MLII'])\n",
        "    data1 = record.p_signal.flatten()\n",
        "\n",
        "    rdata = OneDenoise(data1)\n",
        "\n",
        "    #计算信噪比\n",
        "    S = np.array(rdata) #去噪后的信号\n",
        "    SN = np.array(data1) #原始信号\n",
        "    snr = SNR_singlech(S, SN)\n",
        "    # snr = np.round(snr)#四舍五入\n",
        "    print('snr = ', snr)\n",
        "    \n",
        "    # 计算均方根误差\n",
        "    print(\"rmse = \", math.sqrt(mean_squared_error(data1, rdata)))\n",
        "\n",
        "    # 二维化\n",
        "    bi_gasf = GramTrans(rdata)\n",
        "\n",
        "    rdata2 = TwoDenoise(bi_gasf)\n",
        "\n",
        "    # 画出一维图像\n",
        "    plt.plot(data1)\n",
        "    plt.plot(rdata)\n",
        "    plt.show()\n",
        "\n",
        "    # 画二维\n",
        "    image = bi_gasf[0]\n",
        "    title = 'Summation'\n",
        "    fig, ax = plt.subplots(1, constrained_layout=True)\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(title)\n",
        "    plt.margins(0, 0)\n",
        "    plt.show()\n",
        "\n",
        "    # 画二维去噪图像\n",
        "    image = rdata2[0]\n",
        "    title = 'Summation2'\n",
        "    fig, ax = plt.subplots(1, constrained_layout=True)\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(title)\n",
        "    plt.margins(0, 0)\n",
        "    plt.show()\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "read_ecg_data(data=\"103\")\n"
      ],
      "metadata": {
        "id": "9Sg6teU4LHK2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}